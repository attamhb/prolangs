* 
* 
* UNDERSTANDING DATA TYPES
**  Intro
- check the data type specified for each column in each table
- data dictionary: a document that lists each column; specifies whether it’s a number, character, or other type; and explains the column values.
- Learn by inspecting the table structures in pgAdmin 
- understand data types  
- In a SQL database, each column in a table can hold one and only one data type, which is defined in the CREATE TABLE statement.
- You declare the data type after naming the column. 

#+begin_src sql :result outputs
CREATE TABLE eagle_watch (
observed_date date,
eagles_seen integer
);

#+end_src


1. Characters: Any character or symbol
2. Numbers: Includes whole numbers and fractions
3. Dates and times: Types holding temporal information

** Characters
- Character string types are general-purpose types suitable for any
combination of text, numbers, and symbols. 
- char(n) or character(n): A fixed-length column . 
- varchar(n) or  varying(n): A variable-length column where the maximum length is specified by n. 

** text
A variable-length column of unlimited length. The text type is not part of the SQL standard. The flexibility and potential space savings of varchar and text seem to give them an advantage.  computer.



#+begin_src sql :result outputs

CREATE TABLE char_data_types (
varchar_column varchar(10),
char_column char(10),
text_column text
);
INSERT INTO char_data_types
VALUES
('abc', 'abc', 'abc'),
('defghi', 'defghi', 'defghi');
COPY char_data_types TO 'C:\YourDirectory\typetest.txt'
WITH (FORMAT CSV, HEADER, DELIMITER '|');

#+end_src


- Linux and macOS file paths have a different format. On Linux, my
  desktop is located at /home/anthony/Desktop/. The directory must exist already; PostgreSQL won't create it for you.

- Format the data in the file with each column separated by a pipe character (|). 

- Using varchar with an n value sufficient to handle outliers is a solid strategy.

** Numbers
- Number columns hold various types of  numbers, they also allow you to perform calculations on those numbers.    

- The SQL number types include: Integers Whole numbers, both positive and negative Fixed-point and floating-point Two formats of fractions of whole numbers

** Integers
- These are whole numbers, both positive and negative, including zero.
- The SQL standard provides three integer types: smallint, integer, and bigint.
- The difference between the three types is the maximum size of the numbers they can hold. 

** Auto-Incrementing Integers
- When you add a column with a serial type, PostgreSQL will auto-increment the value in the column each time you insert a row, starting with 1, up to the maximum of each integer type.

- The serial types are implementations of the ANSI SQL standard for auto-numbered identity columns.

- Each database manager implements these in its own way. 
  
#+begin_src sql :result outputs
CREATE TABLE people (
id serial,
person_name varchar(100)
);
#+end_src



- makers of databases often employ a serial type to create a unique ID number, also known as a key, for each row in the table.
- Each row then has its own ID that other tables in the database can reference.
- Because the column is auto-incrementing, you don't need to insert a number into that column when adding data; PostgreSQL handles that for you.
  
- Even though a column with a serial type auto-increments each time a row is added, some scenarios will create gaps in the sequence of numbers in the column. If a row is deleted, for example, the value in that row is never replaced. Or, if a row insert is aborted, the sequence for the column will still be incremented.

** Decimal Numbers
- Decimals represent a whole number plus a fraction of a whole number; the fraction is represented by digits following a decimal point.
- In a SQL database, they’re handled by fixed-point and floating-point data types.
** Fixed-Point Numbers
- The fixed-point type, also called the arbitrary precision type, is numeric(precision,scale).
- You give the argument precision as the maximum number of digits to the left and right of the decimal point, and the argument scale as the number of digits allowable on the right of the decimal point.
- Alternately, you can specify this type using decimal(precision,scale).
- Both are part of the ANSI SQL standard.
- If you omit specifying a scale value, the scale will be set to zero; in effect, that creates an integer.
- If you omit specifying the precision and the scale, the database will store values of any precision and scale up to the maximum allowed.
- That’s up to 131,072 digits before the decimal point and 16,383 digits after the decimal point.

- To collect rainfall totals from several local airports—not an unlikely data analysis task. The U.S. National Weather Service provides this data with rainfall typically measured to two decimal places.

- To record rainfall in the database using five digits total (the precision) and two digits maximum to the right of the decimal (the scale), you’d specify it as numeric(5,2).

- The database will always return two digits to the right of the decimal point, even if you don't enter a number that contains two digits. 

** Floating-Point Types
- The two floating-point types are real and double precision.
- The difference between the two is how much data they store.
- The real type allows precision to six decimal digits, and double precision to 15 decimal points of precision, both of which include the number of digits on both sides of the point.
- These floating-point types are also called variable-precision types.
- The database stores the number in parts representing the digits and an exponent—the location where the decimal point belongs.
- So, unlike numeric, where we specify fixed precision and scale, the decimal point in a given column can "float" depending on the number.

** Using Fixed- and Floating-Point Types
- Each type has differing limits on the number of total digits, or precision, it can hold.

- Create a small table and insert a variety of test cases, 

#+begin_src sql :result outputs

CREATE TABLE number_data_types (
numeric_column numeric(20,5),
real_column real,
double_column double precision
);
INSERT INTO number_data_types
VALUES
(.7, .7, .7),
(2.13579, 2.13579, 2.13579),
(2.1357987654, 2.1357987654, 2.1357987654);
SELECT * FROM number_data_types;

#+end_src


** Trouble with Floating-Point Math
- The way computers store floating-point numbers can lead to unintended mathematical errors. 

#+begin_src sql :result outputs
SELECT
numeric_column * 10000000 AS "Fixed",
real_column * 10000000 AS "Float"
FROM number_data_types
WHERE numeric_column = .7;
#+end_src

- Floating-point types are referred to as "inexact."
- The reason floating-point math produces such errors is that the computer attempts to squeeze lots of information into a finite number of bits.
- The storage required by the numeric data type is variable, and depending on the precision and scale specified, numeric can consume considerably more space than the floating-point types.
- If you're working with millions of rows, it’s worth considering whether you can live with relatively inexact floating-point math.

** Choosing Your Number Data Typw
1. Use integers when possible. Unless your data uses decimals, stick with integer types.
2. If you're working with decimal data and need calculations to be exact, choose numeric or its equivalent, decimal. Float types will save space, but the inexactness of floating- point math won't pass muster in many applications. Use them only when exactness is not as important.
3. Choose a big enough number type. Unless you’re designing a database to hold millions of rows, err on the side of bigger. When using numeric or decimal, set the precision large enough to accommodate the number of digits on both sides of the decimal point. With whole numbers, use bigint unless you're absolutely sure column values will be constrained to fit into the smaller integer or smallint types.

** Dates and Times
- Whenever you enter a date into a search form, you're reaping the benefit of databases having an awareness of the current time plus the ability to handle formats for dates, times, and the nuances of the calendar, such as leap years and time zones.
- This is essential for storytelling with data, because the issue of when something occurred is usually as valuable a question as who, what, or how many were involved. PostgreSQL's date and time support includes the four major data.
- *timetamp* records date and time, which are useful for a range of situations you might track.
- The format timestamp with time zone is part of the SQL standard.
- With PostgreSQL you can specify the same data type using timestamptz. timestamp date Records just the date. 
- *time* records just the time. Again, you’ll want to add the with time zone keywords.
- *interval* holds a value representing a unit of time expressed in the format quantity unit. It doesn’t record the start or end of a time period, only its length.
  
#+begin_src sql :result outputs
CREATE TABLE date_time_types (
timestamp_column timestamp with time zone,
interval_column interval
);
INSERT INTO date_time_types
VALUES
('2018-12-31 01:00 EST','2 days'),
('2018-12-31 01:00 -8','1 month'),
('2018-12-31 01:00 Australia/Melbourne','1 century'),
(now(),'1 week');
SELECT * FROM date_time_types;
#+end_src

** Using the interval Data Type in Calculations

#+begin_src sql :result outputs
SELECT
timestamp_column
interval_column,
timestamp_column - interval_column AS new_date
FROM date_time_types;
#+end_src

** Miscellaneous Types
- A Boolean type that stores a value of true or false
- Geometric types that include points, lines, circles, and other two-dimensional objects Network address types, such as IP or MAC addresses
- A Universally Unique Identifier (UUID) type, sometimes used as a unique key value in tables XML and JSON data types that store information in those structured formats.

** Transforming Values from One Type to Another with CAST
The CAST() function only succeeds when the target data type can
accommodate the original value. Casting an integer as text is possible, because the character types can include numbers. Casting text with letters of the alphabet as a number is not.

#+begin_src sql :result outputs
SELECT timestamp_column, CAST(timestamp_column AS varchar(10))
FROM date_time_types;
SELECT numeric_column,
CAST(numeric_column AS integer),
CAST(numeric_column AS varchar(6))
FROM number_data_types;
SELECT CAST(char_column AS integer) FROM char_data_types;
#+end_src

** CAST Shortcut Notation
- PostgreSQL also offers a less-obvious shortcut notation that takes less space: the double colon.
- Insert the double colon in between the name of the column and the data type you want to convert it to. 
#+begin_src sql :result outputs
cast timestamp_column as a varchar:
SELECT timestamp_column, CAST(timestamp_column AS varchar(10))
FROM date_time_types;
SELECT timestamp_column::varchar(10)
FROM date_time_types;

#+end_src
- Double colon is a PostgreSQL-only implementation not found in other SQL variants.

* IMPORTING AND EXPORTING DATA

** Intro

- PostgreSQL can import the data in bulk via its COPY command.

- This command is a PostgreSQL-specific implementation
  
- COPY will also export data from PostgreSQL tables or from the result of a query to a delimited text file.

- Three steps form the outline of most of the imports

- Prep the source data in the form of a delimited text file, create a table to store the data, write a COPY script to perform the import.

** Working with Delimited Text Files

- A delimited text file contains rows of data, and each row represents one row in a table. In each row, a character separates, or delimits, each data column.

#+begin_src csv :result outputs
John,Doe,123 Main St.,Hyde Park,NY,845-555-1212
#+end_src

** Quoting Columns that Contain Delimiters
- Delimited files wrap columns that contain a delimiter character with an arbitrary character called a text qualifier that tells SQL to ignore the delimiter character held within.
- Most of the time in comma-delimited files the text qualifier used is the double quote. 

#+begin_src csv :result outputs
John,Doe,"123 Main St., Apartment 200",Hyde Park,NY,845-555-1212
#+end_src
- PostgreSQL by default ignores delimiters inside double-quoted columns
- You can specify a different text qualifier if your import requires it. 

** Handling Header Rows

- Header row is a single row at the top, or head, of the file that lists the name of each data field.
#+begin_src csv :result outputs
FIRSTNAME,LASTNAME,STREET,CITY,STATE,PHONE
John,Doe,"123 Main St., Apartment 200",Hyde Park,NY,845-555-1212
#+end_src

- The values in the header row identify the data in each column
- PostgreSQL doesn't use the header row, we don’t want that row imported to a table. 
**  Using COPY to Import Data
- To import data from an external file, check out a source CSV file
- Build the table in PostgreSQL to hold the data
- Thereafter, the SQL statement for the import is relatively simple. 
#+begin_src sql :result outputs
COPY table_name
FROM '/Users/anthony/Desktop/my_file.csv'
-- FROM 'C:\YourDirectory\your_file.csv'
WITH (FORMAT CSV, HEADER);
#+end_src
** File format, Presence of a header row, Delimiter, Quote character
- Use the FORMAT format_name option to specify the type of file you're reading or writing. 
- Format names are CSV, TEXT, or BINARY. 
- On import, use HEADER to specify that the source file has a header row.
- You can also specify it longhand as HEADER ON, which tells the database to start importing with the second line of the file, preventing the unwanted import of the header. 
- The delimiter must be a single character and cannot be a carriage return. 
- PostgreSQL uses the double quote, but if the CSV you're importing uses a different character, you can specify it with the QUOTE 'quote_character' option. 
** Importing Census Data Describing Counties
- It contains census data about every county in the United States and is 3,143 rows deep and 91 columns wide.
- Every 10 years, the government conducts a full count of the population—one of several ongoing programs by the Census Bureau to collect demographic data.
- Each household in America receives a questionnaire about each person in it—their age, gender, race, and whether they are Hispanic or not.
- The U.S. Constitution mandates the count to determine how many members from each state make up the U.S. House of Representatives.
- Based on the 2010 Census, for example, Texas gained four seats in the House while New York and Ohio lost two seats each.
- Although apportioning House seats is the count’s main purpose, the data’s also a boon for trend trackers studying the population.
- Download the us_counties_2010.csv file from https://www.nostarch.com/practicalSQL/ and save it to a folder on your computer. 
 #+begin_src csv  :result outputs
NAME,STUSAB,SUMLEV,REGION,DIVISION,STATE,COUNTY --snip--
#+end_src

** Creating the us_counties_2010 Table
- To import it properly, you’ll need to download the full table definition.

#+begin_src sql :result outputs
CREATE TABLE us_counties_2010 (
geo_name varchar(90),
state_us_abbreviation varchar(2),
summary_level varchar(3),
region smallint,
division smallint,
state_fips varchar(2),
county_fips varchar(3),
area_land bigint,
area_water bigint,
population_count_100_percent integer,
housing_unit_count_100_percent integer,
internal_point_lat numeric(10,7),
internal_point_lon numeric(10,7),
p0010001 integer,
p0010002 integer,
p0010003 integer,
p0010004 integer,
p0010005 integer,
--snip--
p0040049 integer,
p0040065 integer,
p0040072 integer,
h0010001 integer,
h0010002 integer,
h0010003 integer
);

SELECT * from us_counties_2010;
#+end_src

** Census Columns and Data Types
labels as opposed to numbers used for math.

** Performing the Census Import with COPY

#+begin_src sql :result outputs
COPY us_counties_2010
FROM 'C:\YourDirectory\us_counties_2010.csv'
WITH (FORMAT CSV, HEADER);
-- Query returned successfully: 3143 rows affected

SELECT * FROM us_counties_2010;

SELECT geo_name, state_us_abbreviation, area_land
FROM us_counties_2010
ORDER BY area_land DESC
LIMIT 3;

SELECT geo_name, state_us_abbreviation, internal_point_lon
FROM us_counties_2010
ORDER BY internal_point_lon DESC
LIMIT 5;
#+end_src
- Use a LIMIT clause to return the number of rows you want

** Importing a Subset of Columns with COPY
- If a CSV file doesn't have data for all the columns in your target database table, you can still import the data you have by specifying which columns are present in the data.
#+begin_src sql :result outputs

CREATE TABLE supervisor_salaries (
town varchar(30),
county varchar(30),
supervisor varchar(30),
start_date date,
salary money,
benefits money
);

COPY supervisor_salaries
FROM 'C:\YourDirectory\supervisor_salaries.csv'
WITH (FORMAT CSV, HEADER);

-- ERROR: missing data for column "start_date"



COPY supervisor_salaries (town, supervisor, salary)
FROM 'C:\YourDirectory\supervisor_salaries.csv'
WITH (FORMAT CSV, HEADER);
#+end_src

** Adding a Default Value to a Column During Import
- Temporary tables exist only until you end your database session. They're handy for performing intermediary operations on data as part of your processing pipeline; 

#+begin_src sql :result outputs

DELETE FROM supervisor_salaries;
  
CREATE TEMPORARY TABLE supervisor_salaries_temp (LIKE supervisor_salaries);
COPY supervisor_salaries_temp (town, supervisor, salary)
FROM 'C:\YourDirectory\supervisor_salaries.csv'
WITH (FORMAT CSV, HEADER);
INSERT INTO supervisor_salaries (town, county, supervisor, salary)
SELECT town, 'Some County', supervisor, salary
FROM supervisor_salaries_temp;
DROP TABLE supervisor_salaries_temp;

#+end_src

- Create a temporary table called supervisor_salaries_temp
- Import the supervisor_salaries.csv file into the temporary table
- Use an INSERT statement to fill the salaries table .
- Employ a SELECT statement to query the temporary table.
- Use DROP TABLE to erase the temporary table 

** Using COPY to Export Data
- The main difference between exporting and importing data with COPY is that rather than using FROM to identify the source data, you use TO for the path and name of the output file.
- You control how much data to export— an entire table, just a few columns, or to fine-tune it even more, the results of a query. 
** Exporting All Data
- The simplest export sends everything in a table to a file.
- The WITH keyword option tells PostgreSQL to include a header row and use the pipe symbol instead of a comma for a delimiter.
- I’ve used the .txt file extension here for two
reasons. First, it demonstrates that you can export to any text file format; second, we’re using a pipe for a delimiter, not a comma. I like to avoid calling files .csv unless they truly have commas as a separator. Remember to change the output directory to your preferred location.

#+begin_src sql :result outputs


COPY us_counties_2010
TO 'C:\YourDirectory\us_counties_export.txt'
WITH (FORMAT CSV, HEADER, DELIMITER '|');
#+end_src
** Exporting Particular Columns

#+begin_src sql :result outputs

COPY us_counties_2010 (geo_name, internal_point_lat, internal_point_lon)
TO 'C:\YourDirectory\us_counties_latlon_export.txt'
WITH (FORMAT CSV, HEADER, DELIMITER '|');

#+end_src

** Exporting Query Results
Additionally, you can add a query to COPY to fine-tune your output. In Listing 4-9 we export the name and state abbreviation of only those counties whose name contains the letters mill in either uppercase or lowercase by using the case-insensitive ILIKE and the % wildcard character we covered in “Using LIKE and ILIKE with WHERE” on page 19.

#+begin_src sql :result outputs

COPY (
SELECT geo_name, state_us_abbreviation
FROM us_counties_2010
WHERE geo_name ILIKE '%mill%'
)
TO 'C:\YourDirectory\us_counties_mill_export.txt'
WITH (FORMAT CSV, HEADER, DELIMITER '|');
#+end_src

** Importing and Exporting Through pgAdmin
* BASIC MATH AND STATS WITH SQL
| Operator | Description                                        |
| +        | Addition                                           |
| -        | Subtraction                                        |
| *        | Multiplication                                     |
| /        | Division (returns the quotient only, no remainder) |
| %        | Modulo (returns just the remainder)                |
| ^        | Exponentiation                                     |
| !        | Factorial                                          |

** Doing Math Across Census Table Columns
**  Finding Percentages of the Whole
- Percentage of the whole, divide the number in question by the total.
- Calculates for each county the percentage of the population that reported their race as Asian
  
#+begin_src sql :result outputs
SELECT geo_name,
state_us_abbreviation AS "st",
(CAST (p0010006 AS numeric(8,1)) / p0010001) * 100 AS "pct_asian"
FROM us_counties_2010
ORDER BY "pct_asian" DESC;
#+end_src

**  Tracking Percent Change

- Calculates which departments had the greatest percentage increase and loss:
#+begin_src sql :result outputs
CREATE TABLE percent_change (
department varchar(20),
spend_2014 numeric(10,2),
spend_2017 numeric(10,2)
);

INSERT INTO percent_change
VALUES
('Building', 250000, 289000),
('Assessor', 178556, 179500),
('Library', 87777, 90001),
('Clerk', 451980, 650000),
('Police', 250000, 223000),
('Recreation', 199000, 195000);

SELECT department,
spend_2014,
spend_2017,
round( (spend_2017 - spend_2014) /
spend_2014 * 100, 1) AS "pct_change"
FROM percent_change;
#+end_src

** Aggregate Functions for Averages and Sums
#+begin_src sql :result outputs
SELECT sum(p0010001) AS "County Sum", 
       round(avg(p0010001), 0) AS "County Average" 
FROM us_counties_2010; 
#+end_src

** Finding the Median with Percentile Functions
#+begin_src sql :result outputs

CREATE TABLE percentile_test (
    numbers integer
);

INSERT INTO percentile_test (numbers) VALUES
    (1), (2), (3), (4), (5), (6);
SELECT
    percentile_cont(.5)
    WITHIN GROUP (ORDER BY numbers),
    percentile_disc(.5)
    WITHIN GROUP (ORDER BY numbers)
FROM percentile_test;

#+end_src

#+begin_src sql :result outputs
SELECT sum(p0010001) AS "County Sum",
       round(avg(p0010001), 0) AS "County Average",
       percentile_cont(.5)
       WITHIN GROUP (ORDER BY p0010001) AS "County Median"
FROM us_counties_2010;
#+end_src

** Finding Other Quantiles with Percentile Functions

#+begin_src sql :result outputs
SELECT percentile_cont(array[.25,.5,.75])
       WITHIN GROUP (ORDER BY p0010001) AS "quartiles"
FROM us_counties_2010;

SELECT unnest(
             percentile_cont(array[.25,.5,.75])
             WITHIN GROUP (ORDER BY p0010001)
             ) AS "quartiles"
FROM us_counties_2010;
#+end_src

** Creating a median() Function


#+begin_src sql :result outputs

CREATE OR REPLACE FUNCTION _final_median(anyarray)
    RETURNS float8 AS
$$
    WITH q AS
    (
      SELECT val
      FROM unnest($1) val
      WHERE VAL IS NOT NULL
      ORDER BY 1
      ),
      cnt AS
      (
      SELECT COUNT(*) AS c FROM q
      )
      SELECT AVG(val)::float8
      FROM
      (
      SELECT val FROM q
      LIMIT 2 - MOD((SELECT c FROM cnt), 2)
      OFFSET GREATEST(CEIL((SELECT c FROM cnt) / 2.0) - 1,0)
      ) q2;
$$
LANGUAGE sql IMMUTABLE;

CREATE AGGREGATE median(anyelement) (
   SFUNC=array_append,
   STYPE=anyarray,
   FINALFUNC=_final_median,
   INITCOND='{}'
);

SELECT sum(p0010001) AS "County Sum",
       round(AVG(p0010001), 0) AS "County Average",
       median(p0010001) AS "County Median",
       percentile_cont(.5)
       WITHIN GROUP (ORDER BY p0010001) AS "50th Percentile"
FROM us_counties_2010;

#+end_src

** Finding the Mode

#+begin_src sql :result outputs
SELECT mode() WITHIN GROUP (ORDER BY p0010001)
FROM us_counties_2010;
#+end_src
* JOINING TABLES IN A RELATIONAL DATABASE

- In a relational model, each table typically holds data on one entity and each row in the table describes one of those entities.

- A process known as a table join allows us to link rows in one table to rows in other tables.

- Relational model, help eliminate duplicate data, are easier to maintain, and provide for increased flexibility 

** Linking Tables Using JOIN

- To connect tables in a query, we use a JOIN ... ON statement 

#+begin_src sql :result outputs
SELECT *
FROM table_a JOIN table_b
ON table_a.key_column = table_b.foreign_key_column
-- ON table_a.key_column >= table_b.foreign_key_column
#+end_src 

- Name a table
- Give the JOIN keyword
- Name a second table
- The ON keyword follows
- Specify the columns to use to match values

** Relating Tables with Key Columns
#+begin_src sql :result outputs

CREATE TABLE departments (
    dept_id bigserial,
    dept varchar(100),
    city varchar(100),
    CONSTRAINT dept_key PRIMARY KEY (dept_id),
    CONSTRAINT dept_city_unique UNIQUE (dept, city)
);

CREATE TABLE employees (
    emp_id bigserial,
    first_name varchar(100),
    last_name varchar(100),
    salary integer,
    dept_id integer REFERENCES departments (dept_id),
    CONSTRAINT emp_key PRIMARY KEY (emp_id),
    CONSTRAINT emp_dept_unique UNIQUE (emp_id, dept_id)
);

INSERT INTO departments (dept, city)
VALUES
    ('Tax', 'Atlanta'),
    ('IT', 'Boston');

INSERT INTO employees (first_name, last_name, salary, dept_id)
VALUES
     ('Nancy', 'Jones', 62500, 1),
     ('Lee', 'Smith', 59300, 1),
     ('Soo', 'Nguyen', 83000, 2),
     ('Janet', 'King', 95000, 2);


#+end_src

** Querying Multiple Tables Using JOIN
- When joining tables in a query, the database connects rows in both tables where the columns you specified for the join have matching values.

- Can use columns from the joined tables to filter results using a WHERE clause

#+begin_src sql :result outputs

SELECT *
FROM employees JOIN departments
ON employees.dept_id = departments.dept_id;

#+end_src

- When you run the query, the results include all values from both tables where values in the dept_id columns match.

** JOIN Types

- *JOIN* Returns rows from both tables where matching values are found in the joined columns of both tables.

- Alternate syntax is *INNER JOIN*.

- *LEFT JOIN* Returns every row from the left table plus rows that match values in the joined column from the right table.

- *RIGHT JOIN* Returns every row from the right table plus rows that match the key values in the key column from the left table. 

- *FULL OUTER JOIN* Returns every row from both tables and matches rows; then joins the rows where values in the joined columns match.

- *CROSS JOIN* Returns every possible combination of rows from both tables.

#+begin_src sql :result outputs
CREATE TABLE schools_left (
id integer CONSTRAINT left_id_key PRIMARY KEY,
left_school varchar(30)
);

CREATE TABLE schools_right (
id integer CONSTRAINT right_id_key PRIMARY KEY,
right_school varchar(30)
);

INSERT INTO schools_left (id, left_school) VALUES
(1, 'Oak Street School'),
(2, 'Roosevelt High School'),
(5, 'Washington Middle School'),
(6, 'Jefferson High School');

INSERT INTO schools_right (id, right_school) VALUES
(1, 'Oak Street School'),
(2, 'Roosevelt High School'),
(3, 'Morrison Elementary'),
(4, 'Chase Magnet Academy'),
(6, 'Jefferson High School');

#+end_src

** Join

#+begin_src sql :result outputs

SELECT *
FROM schools_left LEFT JOIN schools_right
ON schools_left.id = schools_right.id;

#+end_src
** LEFT JOIN and RIGHT JOIN

#+begin_src sql :result outputs

SELECT *
FROM schools_left LEFT JOIN schools_right
ON schools_left.id = schools_right.id;

SELECT *
FROM schools_left RIGHT JOIN schools_right
ON schools_left.id = schools_right.id;

#+end_src

** FULL OUTER JOIN

#+begin_src sql :result outputs
SELECT *
FROM schools_left FULL OUTER JOIN schools_right
ON schools_left.id = schools_right.id;
#+end_src

** CROSS JOIN
#+begin_src sql :result outputs

SELECT *
FROM schools_left CROSS JOIN schools_right;

#+end_src

** Using NULL to Find Rows with Missing Values

- Use NULL to show that the value is unknown
- 
  #+begin_src sql :result outputs
   
    SELECT *
    FROM schools_left LEFT JOIN schools_right
    ON schools_left.id = schools_right.id
    WHERE schools_right.id IS NULL;

  #+end_src

** Three Types of Table Relationships

The three types of table relationships are one to one, one to many, and many to many.

** One-to-One Relationship
...
** One-to-Many Relationship
- In a one-to-many relationship, a key value in the first table will have multiple matching values in the second table's joined column.
** Many-to-One Relationship
- In a many-to-many relationship, multiple rows in the first table will have multiple matching rows in the second table. 
** Selecting Specific Columns in a Join
- 
#+begin_src sql :result outputs

SELECT id
FROM schools_left LEFT JOIN schools_right
ON schools_left.id = schools_right.id;



SELECT schools_left.id,
       schools_left.left_school,
       schools_right.right_school
FROM schools_left LEFT JOIN schools_right
ON schools_left.id = schools_right.id;


#+end_src
Listing 6-10: Querying specific columns in a join
We simply prefix each column name with the table it comes from, and
the rest of the query syntax is the same. The result returns the requested
** Simplifying JOIN Syntax with Table Aliases

- Write more concise code 

#+begin_src sql :result outputs

SELECT lt.id,
lt.left_school,
rt.right_school
FROM schools_left AS lt LEFT JOIN schools_right AS rt
ON lt.id = rt.id;

#+end_src

** Joining Multiple Tables
#+begin_src sql :result outputs
CREATE TABLE schools_enrollment (
        id integer,
        enrollment integer
);

CREATE TABLE schools_grades (
        id integer,
        grades varchar(10)
);

INSERT INTO schools_enrollment (id, enrollment)
VALUES
    (1, 360),
    (2, 1001),
    (5, 450),
    (6, 927);

INSERT INTO schools_grades (id, grades)
VALUES
    (1, 'K-3'),
    (2, '9-12'),
    (5, '6-8'),
    (6, '9-12');


SELECT lt.id, lt.left_school, en.enrollment, gr.grades
FROM schools_left AS lt LEFT JOIN schools_enrollment AS en
ON lt.id = en.id
LEFT JOIN schools_grades AS gr
ON lt.id = gr.id;

#+end_src


** Performing Math on Joined Table Columns

- The math functions we explored in Chapter 5 are just as usable when working with joined tables. We just need to include the table name when referencing a column in an operation, as we did when selecting table columns. If you work with any data that has a new release at regular intervals, you’ll find this concept useful for joining a newly released table to an older one and exploring how values have changed. That’s certainly what I and many journalists do each time a new set of census data is released. We’ll load the new data and try to find patterns in the growth or decline of the population, income, education, and other indicators. Let’s look at how to do this by revisiting the us_counties_2010 table we created in Chapter 4 and loading similar county data from the previous Decennial Census, in 2000, to a new table. Run the code in Listing 6-13, making sure you’ve saved the CSV file somewhere first:
  
#+begin_src sql :result outputs

CREATE TABLE us_counties_2000 (
    geo_name varchar(90),
    state_us_abbreviation varchar(2),
    state_fips varchar(2),
    county_fips varchar(3),
    p0010001 integer,
    p0010002 integer,
    p0010003 integer,
    p0010004 integer,
    p0010005 integer,
    p0010006 integer,
    p0010007 integer,
    p0010008 integer,
    p0010009 integer,
    p0010010 integer,
    p0020002 integer,
    p0020003 integer
);

COPY us_counties_2000
FROM 'C:\YourDirectory\us_counties_2000.csv'
WITH (FORMAT CSV, HEADER);

SELECT c2010.geo_name,
    2010.state_us_abbreviation AS state,
    2010.p0010001 AS pop_2010,
    2000.p0010001 AS pop_2000
    2010.p0010001 - c2000.p0010001 AS raw_change,
    round( (CAST(c2010.p0010001 AS numeric(8,1)) - c2000.p0010001)
    / c2000.p0010001 * 100, 1 ) AS pct_change
FROM us_counties_2010 c2010 INNER JOIN us_counties_2000 c2000
ON c2010.state_fips = c2000.state_fips
AND c2010.county_fips = c2000.county_fips
AND c2010.p0010001 <> c2000.p0010001
ORDER BY pct_change DESC;

#+end_src

*  TABLE DESIGN THAT WORKS FOR YOU
